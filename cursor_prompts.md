# üéØ Gu√≠a de Prompts para Cursor - Meta-Agente

Esta gu√≠a contiene prompts optimizados para trabajar con Cursor AI en el proyecto Meta-Agente.

---

## üèóÔ∏è PROMPTS DE CONSTRUCCI√ìN

### Crear Nueva Plantilla de Agente

```
# TAREA: Crear plantilla para agentes con b√∫squeda de conocimiento (RAG)

CONTEXTO:
- Archivo: agent_templates.py
- Patr√≥n base: generate_agent_with_memory() l√≠neas 72-120
- Nuevo m√©todo: generate_agent_with_knowledge()

REQUISITOS:
1. Importar AgentKnowledge desde agno.knowledge
2. Configurar vector database (usar PgVector por default)
3. Incluir document loaders
4. Seguir mismo patr√≥n de retorno que otros m√©todos

C√ìDIGO ESPERADO:
- Type hints completos
- Docstring descriptivo
- Manejo de spec con .get() y defaults
- C√≥digo generado v√°lido y ejecutable

REFERENCIAS:
- Agno docs sobre knowledge: https://docs.agno.com/agents/knowledge
- Patr√≥n existente: l√≠nea 72 de agent_templates.py
```

### A√±adir Soporte para Nueva Herramienta

```
# TAREA: A√±adir soporte para GitHub Tools

CONTEXTO:
- Archivo: agent_templates.py
- Funciones a modificar: _get_tool_import() y _generate_tools_init()
- Patr√≥n: ver l√≠neas 200-235

IMPLEMENTACI√ìN:
1. En _get_tool_import():
   - A√±adir entrada: 'github': 'from agno.tools.github import GithubTools'
   - Mapear variantes: 'git', 'repository', 'repo'

2. En _generate_tools_init():
   - Detectar 'github' en tool_lower
   - Retornar: 'GithubTools()'

VALIDACI√ìN:
- Crear agente test con herramienta GitHub
- Verificar que el c√≥digo generado compile
- Probar que el import funcione
```

### Mejorar An√°lisis de Solicitudes

```
# TAREA: Hacer el analyzer_agent m√°s inteligente

CONTEXTO:
- Archivo: meta_agent.py
- M√©todo: analyze_request() l√≠nea 38
- Problema actual: A veces hace preguntas redundantes

OBJETIVO:
Mejorar el prompt del analyzer_agent para que:
1. Detecte cuando ya tiene suficiente informaci√≥n
2. Haga preguntas m√°s espec√≠ficas y contextuales
3. Sugiera opciones concretas al usuario
4. No repita preguntas sobre info ya proporcionada

ENFOQUE:
- Mantener estructura actual del prompt
- A√±adir ejemplos de buenas vs malas preguntas
- Incluir l√≥gica para detectar INFO_COMPLETA m√°s efectivamente
- Usar few-shot learning en el prompt

EJEMPLO OUTPUT ESPERADO:
"Entiendo que necesitas un agente financiero. Bas√°ndome en tu descripci√≥n:
1. ¬øPrefieres an√°lisis de acciones individuales o comparaci√≥n de mercados?
2. ¬øQu√© frecuencia de datos: diaria, semanal, o tiempo real?
Responde con los n√∫meros para ir m√°s r√°pido (ej: 1a, 2c)"
```

---

## üîß PROMPTS DE REFACTORIZACI√ìN

### Optimizar Generaci√≥n de C√≥digo

```
# TAREA: Refactorizar sistema de templates para mejor mantenibilidad

PROBLEMA ACTUAL:
- Mucha duplicaci√≥n de c√≥digo entre templates
- L√≥gica de imports repetida en cada m√©todo
- Dif√≠cil a√±adir nuevas features globalmente

PROPUESTA:
1. Extraer l√≥gica com√∫n a m√©todos auxiliares privados:
   - _build_imports(spec) ‚Üí retorna string de imports
   - _build_agent_config(spec) ‚Üí retorna string de configuraci√≥n
   - _build_example_usage(spec) ‚Üí retorna string de ejemplo

2. Refactorizar generate_*_agent() para usar helpers:
   ```python
   def generate_basic_agent(spec: Dict) -> str:
       imports = self._build_imports(spec)
       config = self._build_agent_config(spec)
       example = self._build_example_usage(spec)
       return self._assemble_code(imports, config, example)
   ```

3. Mantener compatibilidad: No cambiar firmas p√∫blicas

RESULTADO ESPERADO:
- Menos l√≠neas de c√≥digo total
- M√°s f√°cil a√±adir features
- Misma funcionalidad externa
- Tests pasan (cuando los creemos)
```

### Mejorar Manejo de Errores

```
# TAREA: A√±adir manejo robusto de errores

CONTEXTO:
- Archivo: meta_agent.py
- M√©todos: create_plan(), generate_code(), interactive_creation()

PROBLEMAS ACTUALES:
1. JSON parsing puede fallar silenciosamente
2. No hay validaci√≥n de spec antes de generar c√≥digo
3. Errores gen√©ricos poco informativos

SOLUCI√ìN PROPUESTA:
1. En create_plan():
   - Try/catch espec√≠fico para JSONDecodeError
   - Logging del contenido problem√°tico
   - Retry con prompt clarificado
   - Mensaje de error √∫til al usuario

2. En generate_code():
   - Validar spec antes de generar
   - Verificar campos requeridos
   - Custom exceptions: InvalidSpecError, TemplateError

3. En interactive_creation():
   - Wrappear con try/catch comprehensivo
   - Guardar estado en caso de error
   - Ofrecer retry o debug info

PATR√ìN:
```python
try:
    # operaci√≥n
except SpecificError as e:
    logger.error(f"Context: {context}", exc_info=True)
    console.print(f"[red]Error espec√≠fico: {e}[/red]")
    # recovery logic
```
```

---

## üß™ PROMPTS DE TESTING

### Crear Tests Unitarios

```
# TAREA: Crear suite de tests para agent_templates.py

CONTEXTO:
- Framework: pytest
- Archivo nuevo: tests/test_agent_templates.py
- Cobertura objetivo: >80%

ESTRUCTURA:
```python
import pytest
from agent_templates import AgentTemplate

class TestAgentTemplate:
    def test_generate_basic_agent_minimal_spec(self):
        """Test con spec m√≠nimo requerido"""
        spec = {
            'nombre': 'Test Agent',
            'rol': 'Testing',
            'descripcion': 'Test',
            'ejemplo_uso': 'test'
        }
        code = AgentTemplate.generate_basic_agent(spec)
        assert 'Agent(' in code
        assert 'Test Agent' in code
    
    def test_generate_basic_agent_with_tools(self):
        """Test con herramientas"""
        # ...
    
    def test_get_model_import_claude(self):
        """Test mapeo de modelos"""
        # ...
```

CASOS A CUBRIR:
1. Specs m√≠nimos vs completos
2. Cada tipo de modelo soportado
3. Cada herramienta soportada
4. Equipos de agentes
5. Edge cases: specs vac√≠os, None values
```

### Crear Tests de Integraci√≥n

```
# TAREA: Tests end-to-end del flujo completo

ARCHIVO: tests/test_integration.py

ESCENARIOS:
1. Flujo completo: solicitud ‚Üí plan ‚Üí c√≥digo ‚Üí ejecuci√≥n
2. M√∫ltiples iteraciones de preguntas
3. Generaci√≥n de diferentes tipos de agentes
4. Manejo de solicitudes ambiguas

EJEMPLO:
```python
def test_full_workflow_web_search_agent():
    """Test generaci√≥n de agente de b√∫squeda web"""
    meta = MetaAgent()
    
    # Simular conversaci√≥n
    conversation = """
    Usuario: Necesito un agente que busque en internet
    Asistente: ¬øQu√© tipo de b√∫squedas?
    Usuario: Noticias de tecnolog√≠a, sin memoria
    """
    
    plan = meta.create_plan(conversation)
    
    assert plan.nombre is not None
    assert 'duckduckgo' in plan.herramientas
    assert plan.necesita_memoria == False
    
    code = meta.generate_code(plan)
    
    # Verificar que el c√≥digo es v√°lido
    compile(code, '<string>', 'exec')
    
    # Verificar estructura esperada
    assert 'from agno.agent import Agent' in code
    assert 'DuckDuckGoTools' in code
```
```

---

## üé® PROMPTS DE UI/UX

### Mejorar Interfaz CLI

```
# TAREA: Hacer la interfaz m√°s amigable y visual

CONTEXTO:
- Archivo: meta_agent.py ‚Üí interactive_creation()
- Librer√≠a: Rich (ya instalada)

MEJORAS PROPUESTAS:
1. Progress bar durante generaci√≥n:
   ```python
   with console.status("[bold green]Analizando solicitud...") as status:
       analysis = self.analyze_request(conversation)
   ```

2. Tabla para mostrar el plan:
   ```python
   from rich.table import Table
   table = Table(title="Plan del Agente")
   table.add_column("Campo", style="cyan")
   table.add_column("Valor", style="green")
   table.add_row("Nombre", plan.nombre)
   # ...
   console.print(table)
   ```

3. Syntax highlighting para c√≥digo:
   ```python
   from rich.syntax import Syntax
   syntax = Syntax(code, "python", theme="monokai")
   console.print(syntax)
   ```

4. Prompt estilo wizard:
   - N√∫meros de paso m√°s visuales
   - Opciones con letras (a, b, c)
   - Help text contextual

REFERENCIAS:
- Rich docs: https://rich.readthedocs.io/en/latest/
```

### A√±adir Modo Interactivo Avanzado

```
# TAREA: Crear modo wizard con selecci√≥n visual

FUNCIONALIDAD:
- Usar rich.prompt.Prompt para inputs mejorados
- Men√∫ de selecci√≥n con rich.prompt.Confirm
- Questionary-style para opciones m√∫ltiples

EJEMPLO:
```python
from rich.prompt import Prompt, Confirm, IntPrompt
from rich.console import Console

console = Console()

# Tipo de agente
agent_type = Prompt.ask(
    "Tipo de agente",
    choices=["individual", "equipo", "workflow"],
    default="individual"
)

# Herramientas (m√∫ltiple selecci√≥n)
console.print("\n[bold]Herramientas disponibles:[/bold]")
available_tools = [
    "1. B√∫squeda web (DuckDuckGo)",
    "2. Finanzas (YFinance)",
    "3. Razonamiento (Reasoning)",
    "4. C√≥digo (Python)",
]
for tool in available_tools:
    console.print(f"  {tool}")

tools_input = Prompt.ask(
    "\nSelecciona herramientas (n√∫meros separados por coma)",
    default="1"
)
```
```

---

## üìä PROMPTS DE AN√ÅLISIS

### Analizar Calidad del C√≥digo Generado

```
# TAREA: Auditar calidad del c√≥digo que genera el meta-agente

AN√ÅLISIS REQUERIDO:
1. ¬øEl c√≥digo generado sigue PEP 8?
2. ¬øTiene docstrings adecuados?
3. ¬øManeja errores apropiadamente?
4. ¬øEs eficiente y no tiene redundancias?
5. ¬øEs seguro (no ejecuta c√≥digo arbitrario)?

M√âTODO:
1. Generar 10 agentes diferentes
2. Revisar el c√≥digo producido
3. Identificar patrones problem√°ticos
4. Sugerir mejoras a las templates

OUTPUT ESPERADO:
- Reporte con issues encontrados
- Ranking de severidad
- Propuestas de fix
- C√≥digo refactorizado si aplica
```

### Optimizar Performance

```
# TAREA: Profiling del meta-agente

OBJETIVO: Identificar bottlenecks

PASOS:
1. Usar cProfile para medir:
   ```python
   import cProfile
   profiler = cProfile.Profile()
   profiler.enable()
   # c√≥digo del meta-agente
   profiler.disable()
   profiler.print_stats(sort='cumulative')
   ```

2. √Åreas a revisar:
   - Tiempo de respuesta del analyzer_agent
   - Parsing de JSON
   - Generaci√≥n de templates
   - File I/O

3. Optimizaciones posibles:
   - Cache de respuestas comunes
   - Async para llamadas a LLM
   - Lazy loading de templates
   - Memoizaci√≥n de funciones puras

DELIVERABLE:
- Reporte de profiling
- Lista de optimizaciones
- Implementaci√≥n de mejoras
- Benchmarks antes/despu√©s
```

---

## üöÄ PROMPTS DE FEATURES NUEVAS

### A√±adir Modo Batch

```
# TAREA: Crear modo batch para generar m√∫ltiples agentes

CONTEXTO:
- Archivo nuevo: batch_generator.py
- Input: YAML o JSON con m√∫ltiples specs
- Output: M√∫ltiples archivos .py

DISE√ëO:
```python
# agents_batch.yaml
agents:
  - nombre: "Buscador Web"
    rol: "Buscar informaci√≥n"
    herramientas: ["duckduckgo"]
    
  - nombre: "Analista Financiero"
    rol: "Analizar acciones"
    herramientas: ["yfinance"]
```

```python
# batch_generator.py
from meta_agent import MetaAgent
import yaml

def generate_from_batch(yaml_file: str):
    with open(yaml_file) as f:
        config = yaml.safe_load(f)
    
    meta = MetaAgent()
    results = []
    
    for spec in config['agents']:
        plan = AgentPlan(**spec)
        code = meta.generate_code(plan)
        # guardar y reportar
        results.append(...)
    
    return results
```

USO:
```bash
python batch_generator.py agents_batch.yaml --output ./generated/
```
```

### A√±adir Validaci√≥n Pre-generaci√≥n

```
# TAREA: Validar specs antes de generar c√≥digo

PROBLEMA:
- A veces se generan agentes con configuraciones inv√°lidas
- No hay validaci√≥n de compatibilidad herramientas-modelo
- Specs incompletos pasan sin warning

SOLUCI√ìN:
Crear AgentValidator en nuevo archivo validators.py:

```python
class AgentValidator:
    """Valida especificaciones de agentes"""
    
    @staticmethod
    def validate_plan(plan: AgentPlan) -> List[ValidationError]:
        errors = []
        
        # Validar nombre
        if not plan.nombre or len(plan.nombre) < 3:
            errors.append(ValidationError("Nombre muy corto"))
        
        # Validar compatibilidad herramientas-modelo
        if plan.modelo == "claude" and "browser" in plan.herramientas:
            errors.append(ValidationError(
                "Claude no soporta browser tools directamente"
            ))
        
        # Validar nivel vs features
        if plan.nivel == 1 and plan.necesita_memoria:
            errors.append(ValidationError(
                "Nivel 1 no soporta memoria, usar nivel 3+"
            ))
        
        return errors
```

INTEGRACI√ìN:
- Llamar en generate_code() antes de generar
- Mostrar errores al usuario
- Sugerir correcciones
```

---

## ü§ñ PROMPTS META (Para Mejorar Este Sistema)

### Auto-mejora del Meta-Agente

```
# TAREA: Hacer que el meta-agente se pueda auto-mejorar

CONCEPTO:
El meta-agente deber√≠a poder:
1. Analizar agentes que ha generado
2. Identificar patrones comunes
3. Sugerir mejoras a sus propias templates
4. Generar nuevas templates autom√°ticamente

IMPLEMENTACI√ìN:
```python
class SelfImprovingMetaAgent(MetaAgent):
    def analyze_generated_agents(self, agents_dir: str):
        """Analiza agentes generados para encontrar patrones"""
        patterns = []
        # leer todos los .py
        # extraer configuraciones comunes
        # identificar oportunidades de templates nuevos
        return patterns
    
    def suggest_template_improvements(self):
        """Sugiere mejoras basadas en uso real"""
        # analizar qu√© features se usan m√°s
        # detectar configuraciones repetidas
        # proponer nuevas abstracciones
        pass
```

META: Este es un prompt para que Cursor te ayude a hacer el sistema m√°s inteligente
```

---

## üìñ GU√çA DE USO DE ESTOS PROMPTS

### C√≥mo Usar Efectivamente

1. **Copia el prompt completo** - No lo modifiques, est√° optimizado
2. **P√©galo en Cursor** - Usa Cmd/Ctrl+L para abrir chat
3. **A√±ade contexto espec√≠fico** si es necesario
4. **Revisa el c√≥digo generado** - Cursor no es infalible
5. **Itera** - Si no funciona a la primera, refina el prompt

### Estructura de un Buen Prompt

```
# TAREA: [Qu√© hacer - espec√≠fico]

CONTEXTO: [D√≥nde, qu√© archivos, qu√© estado actual]

REQUISITOS: [Lista numerada de lo que debe cumplir]

C√ìDIGO/EJEMPLO: [Si aplica, c√≥digo de referencia]

REFERENCIAS: [Links o l√≠neas de c√≥digo relevantes]
```

### Tips para Cursor

- **S√© espec√≠fico**: "Modifica l√≠nea 42" > "Mejora el c√≥digo"
- **Da ejemplos**: Muestra lo que quieres
- **Referencia c√≥digo existente**: "Como en l√≠nea X"
- **Especifica constraints**: "Sin cambiar la API p√∫blica"
- **Pide explicaciones**: "¬øPor qu√© este approach?"

---

## üéì EJERCICIOS PR√ÅCTICOS

Para familiarizarte con el proyecto, intenta estos ejercos usando Cursor:

### Nivel B√°sico
1. A√±adir soporte para una nueva herramienta simple
2. Mejorar un mensaje de error
3. A√±adir un campo nuevo a AgentPlan

### Nivel Intermedio
4. Crear una nueva plantilla de agente
5. Refactorizar una funci√≥n duplicada
6. A√±adir tests para un m√©todo

### Nivel Avanzado
7. Implementar modo batch
8. A√±adir API REST
9. Crear sistema de plugins para templates

---

**¬øNecesitas m√°s prompts?** P√≠dele a Cursor:
```
Bas√°ndote en CURSOR_PROMPTS.md, genera un prompt para [tu tarea espec√≠fica]
```

**√öltima actualizaci√≥n:** 2025-01-14